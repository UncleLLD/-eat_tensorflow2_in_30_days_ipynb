{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2 训练模型的3种方法\n",
    "模型的训练主要有内置fit方法、内置tran_on_batch方法、自定义训练循环。\n",
    "\n",
    "注：fit_generator方法在tf.keras中不推荐使用，其功能已经被fit包含。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "\n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "\n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train, y_train), (x_test, y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max() + 1\n",
    "CAT_NUM = y_train.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、内置fit方法\n",
    "该方法功能非常强大, 支持对numpy array, tf.data.Dataset以及 Python generator数据进行训练。\n",
    "\n",
    "并且可以通过设置回调函数实现对训练过程的复杂控制逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 281 steps, validate for 71 steps\n",
      "Epoch 1/6\n",
      "281/281 [==============================] - 14s 48ms/step - loss: 2.0336 - sparse_categorical_accuracy: 0.4619 - sparse_top_k_categorical_accuracy: 0.7483 - val_loss: 1.7204 - val_sparse_categorical_accuracy: 0.5548 - val_sparse_top_k_categorical_accuracy: 0.7551\n",
      "Epoch 2/6\n",
      "281/281 [==============================] - 11s 38ms/step - loss: 1.4902 - sparse_categorical_accuracy: 0.6104 - sparse_top_k_categorical_accuracy: 0.8037 - val_loss: 1.5358 - val_sparse_categorical_accuracy: 0.6077 - val_sparse_top_k_categorical_accuracy: 0.7939\n",
      "Epoch 3/6\n",
      "281/281 [==============================] - 10s 37ms/step - loss: 1.1869 - sparse_categorical_accuracy: 0.6858 - sparse_top_k_categorical_accuracy: 0.8597 - val_loss: 1.5546 - val_sparse_categorical_accuracy: 0.6296 - val_sparse_top_k_categorical_accuracy: 0.8068\n",
      "Epoch 4/6\n",
      "281/281 [==============================] - 11s 38ms/step - loss: 0.8925 - sparse_categorical_accuracy: 0.7669 - sparse_top_k_categorical_accuracy: 0.9186 - val_loss: 1.7242 - val_sparse_categorical_accuracy: 0.6398 - val_sparse_top_k_categorical_accuracy: 0.8170\n",
      "Epoch 5/6\n",
      "281/281 [==============================] - 10s 37ms/step - loss: 0.6263 - sparse_categorical_accuracy: 0.8430 - sparse_top_k_categorical_accuracy: 0.9584 - val_loss: 1.9944 - val_sparse_categorical_accuracy: 0.6416 - val_sparse_top_k_categorical_accuracy: 0.8183\n",
      "Epoch 6/6\n",
      "281/281 [==============================] - 10s 37ms/step - loss: 0.4542 - sparse_categorical_accuracy: 0.8926 - sparse_top_k_categorical_accuracy: 0.9743 - val_loss: 2.3567 - val_sparse_categorical_accuracy: 0.6371 - val_sparse_top_k_categorical_accuracy: 0.8179\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train, validation_data=ds_test, epochs=6, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、内置train_on_batch方法\n",
    "该内置方法相比较fit方法更加灵活，可以不通过回调函数而直接在批次层次上更加精细地控制训练的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, ds_train, ds_valid, epochs):\n",
    "    for epoch in tf.range(1, epochs+1):\n",
    "        model.reset_metrics()\n",
    "        \n",
    "        # 在后期降低学习率\n",
    "        if epoch == 5:\n",
    "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
    "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
    "            \n",
    "        for x, y in ds_train:\n",
    "            train_result = model.train_on_batch(x, y)\n",
    "            \n",
    "        for x,y in ds_valid:\n",
    "            valid_result = model.test_on_batch(x, y)\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch = \",epoch)\n",
    "            print(\"train:\", dict(zip(model.metrics_names, train_result)))\n",
    "            print(\"valid:\", dict(zip(model.metrics_names, valid_result)))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================21:01:09\n",
      "epoch =  1\n",
      "train: {'loss': 0.03985308, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 4.4741173, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n",
      "================================================================================21:01:18\n",
      "epoch =  2\n",
      "train: {'loss': 0.033269268, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 4.78131, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n",
      "================================================================================21:01:26\n",
      "epoch =  3\n",
      "train: {'loss': 0.026025575, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 5.027471, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n",
      "================================================================================21:01:35\n",
      "epoch =  4\n",
      "train: {'loss': 0.022150518, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 5.1505265, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n",
      "Lowering optimizer Learning Rate...\n",
      "\n",
      "\n",
      "================================================================================21:01:43\n",
      "epoch =  5\n",
      "train: {'loss': 0.028875617, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 4.292439, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n",
      "================================================================================21:01:51\n",
      "epoch =  6\n",
      "train: {'loss': 0.019162333, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 4.3197393, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n",
      "================================================================================21:01:59\n",
      "epoch =  7\n",
      "train: {'loss': 0.017979981, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 4.2962976, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n",
      "================================================================================21:02:07\n",
      "epoch =  8\n",
      "train: {'loss': 0.016550334, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 4.25021, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n",
      "================================================================================21:02:16\n",
      "epoch =  9\n",
      "train: {'loss': 0.015594009, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 4.1997952, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n",
      "================================================================================21:02:24\n",
      "epoch =  10\n",
      "train: {'loss': 0.014656758, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 4.1804786, 'sparse_categorical_accuracy': 0.33333334, 'sparse_top_k_categorical_accuracy': 0.6666667}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, ds_train, ds_test, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、自定义训练循环\n",
    "自定义训练循环无需编译模型，直接利用优化器根据损失函数反向传播迭代参数，拥有最高的灵活性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================21:02:34\n",
      "Epoch=1,Loss:0.184110194,Accuracy:0.949343145,Valid Loss:3.47809172,Valid Accuracy:0.63089937\n",
      "\n",
      "================================================================================21:02:43\n",
      "Epoch=2,Loss:0.17708835,Accuracy:0.946671128,Valid Loss:3.6346848,Valid Accuracy:0.628673196\n",
      "\n",
      "================================================================================21:02:51\n",
      "Epoch=3,Loss:0.163738102,Accuracy:0.94900912,Valid Loss:3.66780066,Valid Accuracy:0.629118443\n",
      "\n",
      "================================================================================21:02:58\n",
      "Epoch=4,Loss:0.150552884,Accuracy:0.949899793,Valid Loss:3.69140673,Valid Accuracy:0.626447\n",
      "\n",
      "================================================================================21:03:06\n",
      "Epoch=5,Loss:0.143080384,Accuracy:0.951013148,Valid Loss:3.66971564,Valid Accuracy:0.623330355\n",
      "\n",
      "================================================================================21:03:16\n",
      "Epoch=6,Loss:0.135870054,Accuracy:0.952126503,Valid Loss:3.71025276,Valid Accuracy:0.623775601\n",
      "\n",
      "================================================================================21:03:23\n",
      "Epoch=7,Loss:0.127501383,Accuracy:0.955466509,Valid Loss:3.76818109,Valid Accuracy:0.621549428\n",
      "\n",
      "================================================================================21:03:32\n",
      "Epoch=8,Loss:0.120428041,Accuracy:0.956579804,Valid Loss:3.81625462,Valid Accuracy:0.616651833\n",
      "\n",
      "================================================================================21:03:39\n",
      "Epoch=9,Loss:0.116922945,Accuracy:0.958583832,Valid Loss:3.90766406,Valid Accuracy:0.614870906\n",
      "\n",
      "================================================================================21:03:48\n",
      "Epoch=10,Loss:0.110454135,Accuracy:0.959363163,Valid Loss:3.91532326,Valid Accuracy:0.610418499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
